# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.optim as optim
# import pytorch_lightning as pl
# from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping


# class AE(nn.Module):
#     def __init__(self, input_channels=3, latent_channels=4, num_embeddings=512):
#         super().__init__()
#         # Encoder
#         self.encoder = nn.Sequential(
#             nn.Conv2d(input_channels, 64, 3, stride=2, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(64, 128, 3, stride=2, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(128, 256, 3, stride=1, padding=1),
#             nn.ReLU()
#         )

#         # Pre-quantization convolution
#         self.pre_quant_conv = nn.Conv2d(256, latent_channels, 1)

#         # Decoder
#         self.post_quant_conv = nn.Conv2d(latent_channels, 256, 1)
#         self.decoder = nn.Sequential(
#             nn.Conv2d(256, 128, 3, padding=1),
#             nn.ReLU(),
#             nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
#             nn.ReLU(),
#             nn.ConvTranspose2d(64, input_channels, 4, stride=2, padding=1),
#             nn.Sigmoid()
#         )

#     def preprocess(self, x):
#         return 2 * x - 1  # [0, 1] -> [-1, 1]

#     def encode(self, x, return_multiple=False):
#         x = self.preprocess(x)
#         h = self.encoder(x)
#         z = self.pre_quant_conv(h)
#         return z

#     def decode(self, z_q):
#         h = self.post_quant_conv(z_q)
#         return self.decoder(h)

#     def forward(self, x):
#         z = self.encode(x)
#         x_recon = self.decode(z)
#         return x_recon, z, None, None

#     def get_latent_vector(self, x):
#         assert not self.training
#         z_q = self.encode(x)
#         return z_q.reshape(z_q.size(0), -1)

# class AELightning(pl.LightningModule):
#     def __init__(self, input_channels=3, latent_channels=4, lr=1e-3):
#         super().__init__()
#         self.save_hyperparameters()
#         self.model = AE(input_channels, latent_channels)

#     def forward(self, x):
#         return self.model(x)

#     def training_step(self, batch, batch_idx):
#         x, _ = batch
#         x_recon, z, _, _ = self.model(x)
#         recon_loss = F.mse_loss(x, x_recon)

#         self.log_dict({
#             "train_recon": recon_loss,
#         }, prog_bar=True)
#         return recon_loss

#     def validation_step(self, batch, batch_idx):
#         x, _ = batch
#         x_recon, z, _, _ = self.model(x)
#         recon_loss = F.mse_loss(x, x_recon)

#         self.log_dict({
#             "val_recon": recon_loss,
#         })

#     def configure_optimizers(self):
#         optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)
#         scheduler = optim.lr_scheduler.CosineAnnealingLR(
#             optimizer, 
#             T_max=20,
#             eta_min=1e-4
#         )
#         return {
#             "optimizer": optimizer,
#             "lr_scheduler": {
#                 "scheduler": scheduler,
#                 "interval": "epoch",
#                 "frequency": 1
#             }
#         }

#     def configure_callbacks(self):
#         return [
#             ModelCheckpoint(
#                 monitor="val_recon",
#                 filename="best",
#                 save_top_k=1,
#                 mode="min",
#                 save_last=True
#             ),
#             LearningRateMonitor(logging_interval='step'),
#             # EarlyStopping(monitor="val_recon", mode="min", patience=10)
#         ]



# import torch
# import os

# ckpt_path= "/root/model_ae/lightning_logs/version_1/checkpoints/last.ckpt"
# model = AELightning.load_from_checkpoint(ckpt_path)

# # 提取 PyTorch 模型本体（即 self.model）
# torch_model = model.model

# # 自动保存在 submit 目录
# save_dir = "submit"
# os.makedirs(save_dir, exist_ok=True)
# save_path = os.path.join(save_dir, "your_model_version1.pt")

# # 保存模型参数
# torch.save(torch_model.state_dict(), save_path)

# print(f"✅ .pt 文件已保存：{save_path}")


##################################################################################################################################
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
import math



class AE(nn.Module):
    def __init__(self, input_channels=3, latent_channels=4, num_embeddings=512):
        super().__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(input_channels, 64, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU()
        )

        # Pre-quantization convolution
        self.pre_quant_conv = nn.Conv2d(256, latent_channels, 1)

        # Decoder
        self.post_quant_conv = nn.Conv2d(latent_channels, 256, 1)
        self.decoder = nn.Sequential(
            nn.Conv2d(256, 128, 3, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, input_channels, 4, stride=2, padding=1),
            nn.Sigmoid()
        )

    def preprocess(self, x):
        return 2 * x - 1  # [0, 1] -> [-1, 1]

    def encode(self, x, return_multiple=False):
        x = self.preprocess(x)
        h = self.encoder(x)
        z = self.pre_quant_conv(h)
        return z

    def decode(self, z_q):
        h = self.post_quant_conv(z_q)
        return self.decoder(h)

    def forward(self, x):
        z = self.encode(x)
        x_recon = self.decode(z)
        return x_recon, z, None, None

    def get_latent_vector(self, x):
        assert not self.training
        z_q = self.encode(x)
        return z_q.reshape(z_q.size(0), -1)
    


def probe_warmup(epoch, max_val=0.005, total_epochs=200):
    return max_val * (1 - math.exp(-epoch / (total_epochs * 0.3)))

class ProbingClassifier(nn.Module):
    def __init__(self, in_channels, num_classes=170):
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(in_channels, num_classes)

    def forward(self, z):
        z = self.pool(z).squeeze(-1).squeeze(-1)  # (B, C)
        return self.fc(z)


class AELightning(pl.LightningModule):
    def __init__(self, input_channels=3, latent_channels=8, lr=1e-3, z_dim=4096, num_classes=170):
        super().__init__()
        # self.save_hyperparameters()
        # self.model = Model(input_channels, latent_channels)
        # self.probing_head = ProbingClassifier(latent_channels, num_classes)
        super().__init__()
        self.save_hyperparameters()
        self.model = AE(input_channels, latent_channels)
        self.probing_head = ProbingClassifier(latent_channels, num_classes)


    def forward(self, x):
        x_recon, z, _, _ = self.model(x)
        return x_recon, z

    def training_step(self, batch, batch_idx):
        x, y = batch  
        x_recon, z = self.forward(x)

        # MSE + CrossEntropy
        recon_loss = F.mse_loss(x_recon, x, reduction='mean')
        logits = self.probing_head(z)
        probe_loss = F.cross_entropy(logits, y)

        alpha = probe_warmup(self.current_epoch, max_val=0.1, total_epochs=self.trainer.max_epochs)
        total_loss = recon_loss + alpha * probe_loss

        acc = (logits.argmax(dim=1) == y).float().mean()

        self.log_dict({
            "train_mse": recon_loss,
            "train_probe_loss": probe_loss,
            "train_probe_acc": acc,
            "alpha": alpha
        }, prog_bar=False)

        return total_loss
    
    def validation_step(self, batch, batch_idx):
        x, y = batch
        x_recon, z = self.forward(x)

        recon_loss = F.mse_loss(x_recon, x, reduction='mean')
        logits = self.probing_head(z)
        acc = (logits.argmax(dim=1) == y).float().mean()

        self.log("val_mse", recon_loss, prog_bar=True)
        self.log("val_probe_acc", acc, prog_bar=True)

        safe_acc = acc if recon_loss < 0.0007 else torch.tensor(-1.0, device=self.device)
        self.log("val_probe_acc_safe", safe_acc, prog_bar=False)

        return recon_loss


    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)  # 包括 probing head
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-5)
        return {"optimizer": optimizer, "lr_scheduler": scheduler}
  



import torch
import os

ckpt_path= "/root/model_ae/lightning_logs/version_2/checkpoints/epoch=99-step=12200.ckpt"
model = AELightning.load_from_checkpoint(ckpt_path)

# 提取 PyTorch 模型本体（即 self.model）
torch_model = model.model

# 自动保存在 submit 目录
save_dir = "submit"
os.makedirs(save_dir, exist_ok=True)
save_path = os.path.join(save_dir, "your_model_version2_add_celoss.pt")

# 保存模型参数
torch.save(torch_model.state_dict(), save_path)

print(f"✅ .pt 文件已保存：{save_path}")

